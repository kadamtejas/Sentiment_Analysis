# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VzvpXomXcCCGZFP2M0Ns7kP53dOtqeFs
"""

import pandas as pd
import numpy as np
import nltk
import warnings
warnings.filterwarnings('ignore')

nltk.download('punkt')       # Tokenizer
nltk.download('stopwords')   # Common stopwords
nltk.download('wordnet')     # For lemmatization
nltk.download('omw-1.4')     # Lemmatizer support

def clean_text(dataframe_name, column_name="text"):
    dataframe_name[column_name] = dataframe_name[column_name].str.replace(r"#\w+", "", regex=True)    # remove hashtags
    dataframe_name[column_name] = dataframe_name[column_name].str.replace(r"@\w+", "", regex=True)    # remove mentions
    dataframe_name[column_name] = dataframe_name[column_name].str.replace(r"http\S+", "", regex=True) # remove URLs
    dataframe_name[column_name] = dataframe_name[column_name].str.replace(r"[^\w\s]", "", regex=True) # remove punctuation
    dataframe_name[column_name] = dataframe_name[column_name].str.replace(r"\s+", " ", regex=True)    # normalize spaces
    dataframe_name[column_name] = dataframe_name[column_name].str.strip()                             # remove leading/trailing spaces
    dataframe_name[column_name] = dataframe_name[column_name].str.lower()                             # lowercase
    return dataframe_name

#Dataframe 1 preview
df1 = pd.read_csv(r"/content/Tweets.csv")
df1.head()
df1 = df1[['text','airline_sentiment']]
df1 = clean_text(df1,"text")

df2 = pd.read_csv(r"/content/Twitter_Data.csv")
df2.head()
df2.rename(columns={'clean_text': 'text','category':'airline_sentiment'}, inplace=True)
df2 = clean_text(df2,"text")

df3 = pd.read_csv(r"/content/twitter_training.csv", header = None, names=["id","area","airline_sentiment","text"])
df3.drop(columns=['id','area'], inplace=True)
df3 = clean_text(df3,"text")

# Appending all 3 dataframes
df1['airline_sentiment'].replace("positive",2,inplace=True)
df1['airline_sentiment'].replace("negative",0,inplace=True)
df1['airline_sentiment'].replace("neutral",1,inplace=True)
df3['airline_sentiment'].replace("Positive",2,inplace=True)
df3['airline_sentiment'].replace("Negative",0,inplace=True)
df3['airline_sentiment'].replace("Neutral",1,inplace=True)

appended_df = pd.concat([df1,df2,df3], ignore_index=True)
appended_df

df = appended_df.copy()

df = df.rename(columns={"airline_sentiment": "labels"})
df = df.head(10)
df

df = df.dropna(subset=['text','labels'])
df['labels'].replace("Irrelevant",0,inplace=True)
df['labels']= df['labels'].astype(int)
X = df['text']
y = df['labels']

X

!pip install datasets
from datasets import Dataset
dataset = Dataset.from_pandas(df)
dataset

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoConfig

model_name = "finiteautomata/bertweet-base-sentiment-analysis"
num_labels = 3  # because your label classes are 0, 1, 2

tokenizer = AutoTokenizer.from_pretrained(model_name)

# Define config with correct number of labels
config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)

# Load the model with the config
bert_model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)

# üî¥ Don't use pipeline before training! Comment this out:
# nlp_pipeline = pipeline("sentiment-analysis", model=bert_model, tokenizer=tokenizer)
# print(nlp_pipeline("He is a guy"))  # ‚ùå Will give garbage result ‚Äî model not trained

def tokenize_function(example):
  return tokenizer(example["text"],padding="max_length",truncation=True)

tokenized_dataset = dataset.map(tokenize_function,batched=True)
# tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

#Now splitting data into train and test

from datasets import DatasetDict

train_testvalid = tokenized_dataset.train_test_split(test_size=0.2)

train_dataset = train_testvalid["train"]
test_dataset = train_testvalid["test"]

from torch.utils.data import DataLoader
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(test_dataset, batch_size=8)



from transformers import TrainingArguments, Trainer
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=50,
    weight_decay=0.01,
    push_to_hub=False,
    report_to="none",
)

trainer = Trainer(
    model=bert_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
)

import os
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

trainer.train()

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

model_path = "./results/checkpoint-50"  # folder with your files

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

nlp_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

tweets = [
    "I love this flight experience!",
    "The flight was on time hours.",
    "Best experience.",
    "bad"
]

preds = nlp_pipeline(tweets)
print(preds)

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Save model and tokenizer
model.save_pretrained("sentiment_model")
tokenizer.save_pretrained("sentiment_model")

!zip -r sentiment_model.zip sentiment_model
from google.colab import files
files.download("sentiment_model.zip")

